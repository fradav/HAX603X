---
title: "Théorèmes asymptotiques"
format:
  html:
    out.width: 50%
filters:
  - shinylive
---


## Loi des grands nombres


Le premier résultat fondamental en probabilités concerne le comportement asymptotique de la moyenne empirique:
$$
\bar X_n = \dfrac{X_1 + \cdots + X_n}{n} \enspace.
$$
quand on obsever $n$ variables aléatoires i.i.d $X_1,\dots,X_n$, ayant une espérance finie.

:::: {#thm-lfgn}

## Loi forte des grands nombres

<br>



Soit $(X_n)_{n \geq 1}$ une suite de variables aléatoires indépendantes et identiquement distribuées (i.i.d.) dans $L^1(\Omega, \mathcal{F}, \mathbb{P})$. Notons $\mu = \mathbb{E}[X_1]$. Alors $\bar X_n$ converge vers $\mu$ presque sûrement :
$$
\mathbb{P}\bigg( \dfrac{X_1 + \cdots + X_n}{n} \underset{n \to \infty}{\longrightarrow} \mu \bigg) = 1\,.
$$

::::


**Interprétation**:
Intuitivement, la probabilité d'un événement $A$ correspond à la fréquence d'apparition de $A$ quand on répète une expérience qui fait intervenir cet événement. Par exemple, si on dispose une pièce truquée, on estimera la probabilité d'apparition du côté pile en lançant la pièce un grand nombre de fois et en comptant le nombre de pile obtenu. La loi des grands nombres justifie a posteriori cette intuition : si $X_1, \ldots, X_n$ sont i.i.d. de loi de Bernoulli de paramètre $p$, alors
$$
	\dfrac{X_1 + \cdots + X_n}{n} \xrightarrow[n \to \infty]{p.s.} p \enspace.
$$
Le membre de gauche correspond au nombre empirique de pile obtenu, celui de droite à la valeur théorique.


**Remarque**: Bien qu'assez intuitif, ce théorème est difficile à démontrer. XXX TODO reference : Williams (en anglais) ou bien Ouvrard.


```{shinylive-python}
#| standalone: true
#| viewerHeight: 550
import numpy as np


from shiny import ui, render, App, reactive
from shinywidgets import output_widget, register_widget
from plotly.subplots import make_subplots

import plotly.graph_objs as go


app_ui = ui.page_fluid(
    ui.panel_title("Loi des grands nombres: visualisation"),
    ui.input_action_button("seed", "Ré-échantillonner", class_="btn-primary"),
    output_widget("my_widget"),
    ui.row(
        ui.column(
            6,
            ui.input_slider(
                "p",
                "Paramètre de Bernoulli: p",
                0.01,
                0.99,
                value=0.5,
                step=0.01,
            ),
        ),
        ui.column(
            6,
            ui.input_slider(
                "n_samples",
                "Nombre de tirages aléatoires ",
                2,
                1000,
                value=15,
                step=1,
            ),
        ),
    ),
)


def server(input, output, session):
    seed = reactive.Value(41)

    @reactive.Effect
    @reactive.event(input.seed)
    def _():
        seed.set(np.random.randint(0, 1000))

    subplots = make_subplots(
        rows=2,
        cols=1,
        vertical_spacing=0.3,
        horizontal_spacing=0.04,
        row_heights=[8, 1],
        subplot_titles=(
            f"Moyenne empirique en fonction du nombre de tirages <br>(loi de Bernoulli)",
            "Tirages aléatoires <span style='color:rgb(66, 139, 202)'>bleu: 0</span>, <span style='color:rgb(255, 0, 0)'>rouge: 1</span> (seed="
            + str(42)
            + ")",
        ),
    )
    fig = go.FigureWidget(subplots)
    fig.add_trace(
        go.Scatter(
            mode="lines",
            line=dict(color="black", width=3),
            x=[],
            y=[],
            name=r"Moyenne <br> empirique",
        ),
        row=1,
        col=1,
    )
    fig.add_trace(
        go.Scatter(
            mode="lines",
            line=dict(dash="dash", color="black", width=1),
            marker={},
            x=[],
            y=[],
            name=r"p",
        ),
        row=1,
        col=1,
    )
    fig.add_trace(
        go.Heatmap(
            x=[],
            z=[],
            colorscale=[[0, "rgb(66, 139, 202)"], [1, "rgb(255,0,0)"]],
            showscale=False,
        ),
        row=2,
        col=1,
    )

    fig.update_yaxes(range=[0, 1.1], row=1, col=1)
    fig.update_xaxes(matches="x1", row=2, col=1)
    fig.update_yaxes(visible=False, row=2, col=1)
    fig.update_xaxes(visible=False, row=2, col=1)

    fig.update_layout(
        template="simple_white",
        showlegend=True,
    )
    fig.update_layout(autosize=True)

    fig.update_layout(
        legend=dict(
            yanchor="top",
            y=0.99,
            xanchor="left",
            x=0.95,
            bgcolor="rgba(0,0,0,0)",
        )
    )

    register_widget("my_widget", fig)

    @reactive.Effect
    def _():
        p = input.p()
        n_samples = input.n_samples()

        rng = np.random.default_rng(seed())
        iterations = np.arange(1, n_samples + 1)
        samples = rng.binomial(1, p, size=n_samples)
        means_samples = np.cumsum(samples) / np.arange(1, n_samples + 1)

        # Update data in fig:
        fig.data[0].x = iterations
        fig.data[0].y = means_samples

        fig.data[1].x = iterations
        fig.data[1].y = np.full((n_samples), p)

        fig.data[2].x = iterations
        fig.data[2].z = [samples]

        fig.update_xaxes(range=[1, n_samples + 1])
        # Update the subplot titles:
        fig.layout.annotations[1].update(
            text=f"Tirages aléatoires <span style='color:rgb(66, 139, 202)'>bleu: 0</span>, <span style='color:rgb(255, 0, 0)'>rouge: 1</span> (seed="
            + f"{seed(): 3}"
            + ")"
        )


app = App(app_ui, server)

```

XXX TODO: phénomène intéressant en bougeant le paramètre p avec  le reste fixé...les signaux générés sont très très proches, ce qui ne devrait pas être le cas a priori, saut structuration particulière de la génération.



## Théorème central limite

Une fois la loi des grands nombres établie, on peut se demander quel est l'ordre suivant dans le développement asymptotique de $\bar X_n - \mu$, ou de manière équivalente de $S_n - n \mu$, où $S_n = X_1 + \cdots + X_n$.
Le théorème suivant répond à cette question, en donnant une convergence en loi d'une transformation affine de la moyenne empirique:

:::: {#thm-tcl}

## Théorème central limite
Soit $X_1, \ldots, X_n$ une suite de variables aléatoires i.i.d de variance $\sigma^2 = {\rm var}(X_1) \in ]0, \infty[$. On note $\mu = \mathbb{E}[X_1]$ leur espérance. Alors
$$
\sqrt n \left(\frac{\bar X_n - \mu}{\sigma} \right) \xrightarrow[n \to +\infty]{\mathcal{L}} N\enspace,
$$
où $N$ suit une loi normale centrée réduite : $N \sim\mathcal{N}(0,1)$.
::::
Preuve XXX TODO: donner une référence.

On peut interpréter ce théorème grossièrement de la façon suivante:
la moyenne empirique de variables aléatoires i.i.d de variance $\sigma^2$
se comporte asymptotiquement comme une loi normale $\mathcal{N}(\mu, \tfrac{\sigma^2}{n})$, ce que l'on écrit avec un abus de notation:

$$
\bar X_n \approx \mathcal{N}(\mu, \frac{\sigma^2}{n}) \enspace.
$$

En termes de somme cumulée empirique, la convergence se réécrit

$$
    \frac{S_n - n \mu}{\sqrt n \sigma} \xrightarrow[n \to +\infty]{\mathcal{L}} N \enspace.
$$

Les hypothèses de ce théorème sont plutôt faibles (il suffit de supposer une variance finie). Pourtant, le résultat est universel : la loi de départ peut être aussi farfelue que l'on veut, elle se rapprochera toujours asymptotiquement d'une loi normale.

On rappelle que la convergence en loi est équivalente à la convergence des fonctions de répartition en tout point de continuité de la limite. Ainsi, le théorème central limite se réécrit de la manière suivante : pour tout $a < b$,

$$
	\mathbb{P} \bigg( a \leq \sqrt n \left(\frac{\bar X_n - \mu}{\sigma} \right) \leq b\bigg)
	\underset{n \to \infty}{\longrightarrow}  \dfrac{1}{\sqrt{2\pi}} \int_a^b e^{-\frac{x^2}{2}} \, \mathrm dx\,.
$$

:::: {#exm-tcl}


On considère des variables aléatoires $X_1, \ldots, X_n$ i.i.d. suivant une loi de Bernoulli de paramètre $p \in ]0,1[$, dont l'espérance et la variance sont respectivemenbt $p$ et $p(1-p)$.
Le théorème central limite donne alors
$$
    \sqrt n \left(\frac{\bar X_n - p}{p (1-p)} \right) \xrightarrow[n \to +\infty]{\mathcal{L}} N\,,
$$
avec $N \sim \mathcal{N}(0,1)$. Cette convergence est illustrée en Figure XXX TODO image.
::::


```{shinylive-python}
#| standalone: true
#| viewerHeight: 600
import numpy as np
from scipy.stats import norm
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from shiny import ui, render, App, reactive
from shinywidgets import output_widget, render_widget


app_ui = ui.page_fluid(
    ui.panel_title("Théorème centrale limite: visualisation"),
    ui.input_action_button("seed", "Ré-échantillonner",class_="btn-primary"),
    output_widget("my_widget"),
    ui.row(
        ui.column(4,
            ui.input_slider("p", "Paramètre de Bernoulli: p", 0.01, 0.99, value=0.5, step=0.01)
        ),
        ui.column(4,
            ui.input_slider("n_repetitions", "Nombre de répétitions", 1, 300, value=200, step=1)
        ),
        # ui.column(4, ui.input_slider("step", "n", 1, n_samples, value=10)),
        ui.column(4, ui.input_slider("n_samples", "Nombre de tirages aléatoires ", 1, 200, value=100, step=1)
        )
    )
)


def server(input, output, session):
    seed = reactive.Value(42)


    @reactive.Effect
    @reactive.event(input.seed)
    def _():
        seed.set(np.random.randint(0, 1000))


    @output
    @render_widget
    def my_widget():

        rng = np.random.default_rng(seed())
        p = input.p()
        n_repetitions = input.n_repetitions()
        n_samples = input.n_samples()
        iterations = np.arange(1, n_samples + 1)
        samples = rng.binomial(1, p, size=(n_repetitions, n_samples))
        means_samples = np.cumsum(samples, axis=1) / np.arange(1, n_samples + 1)
        x_hist = np.linspace(0, 1, num=300)

        # Create figure
        fig = make_subplots(
                    rows=1,
                    cols=3,
                    vertical_spacing=0.2,
                    horizontal_spacing=0.02,
                    column_widths=[3, 0.1, 2],
                    subplot_titles=("Moyenne empirique:<br> évolution après " + str(n_samples) + " tirages", " ", "Histogramme obtenu avec <br>" + str(n_repetitions) + " répétitions" + " (seed="+ str(seed()) + ")")
                )


        for i in range(n_repetitions):
            fig.add_trace(
                    go.Scatter(
                        mode='lines',
                        line=dict(color="rgba(0,0,0,0.05)", width=1),
                        x=iterations,
                        y=means_samples[i,:]
                        ),
                        row=1, col=1,
            )
        fig.add_trace(
                go.Scatter(
                    mode='lines',
                    line=dict(dash="dash", color="black", width=1),
                    marker={},
                    x=iterations,
                    y=np.full((n_samples), p),
                    name='p'),
                    row=1, col=1,
        )
        fig.update_layout(
            template="simple_white",
            showlegend=False,
        )
        fig.add_trace(
                go.Scatter(
                    mode='markers',
                    marker=dict(color="rgba(0,0,0,0.05)", size=4),
                    x=np.zeros(n_samples),
                    y=means_samples[:,-1],
                ),
                row=1, col=2,

        )
        y_hist, bins = np.histogram(means_samples[:,-1], bins=int(np.sqrt(n_repetitions)), density=True)
        fig.add_trace(
            go.Bar(x=y_hist, y=bins[:-1] + np.diff(bins)/2,
                    opacity=0.75,
                    marker_color = 'black',
                    orientation='h',
                    width=np.diff(bins),
                    name="Tirages de moyennes empiriques",
                    ),
                row=1, col=3,
        )
        fig.add_trace(
            go.Scatter(x=norm.pdf(x_hist, p, np.sqrt(p*(1-p) / n_samples)),
                       y=x_hist,
                       mode='lines',
                       line=dict(color="red"),
                       legendgroup='1',
                       name="TCL"
                       ),
                row=1, col=3,
        )

        fig.update_xaxes(range=[1, n_samples + 1])
        fig.update_yaxes(range=[-.05, 1.05], row=1, col=1)
        fig.update_yaxes(matches="y1",visible = False,  row=1, col=2)
        fig.update_xaxes(range=[-0.2, 0.2], visible = False, row=1, col=2)

        fig.update_yaxes(matches="y1", row=1, col=3, visible=False)
        fig.update_xaxes(range=[0, 1.1 / np.sqrt(2*np.pi* p*(1-p) / n_samples)], row=1, col=3)
        fig.update_xaxes(visible=False, row=1, col=3)



        for trace in fig['data']:
            print(trace)
            if (trace['name'] != 'TCL') and (trace['name'] != 'p'):
                trace['showlegend'] = False

        fig.update_layout(
            title=dict(text="Moyennes empiriques et TCL (loi de Bernoulli)<br>", yanchor="top", y=0.95),
            title_x=0.5,
            showlegend=True
        )
        fig.update_layout(autosize=True)

        return fig


app = App(app_ui, server)

```